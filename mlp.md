
# **Άσκηση 1**

## **Παράμετροι και Οδηγίες Εκτέλεσης**

Στην άσκηση αυτή υλοποιήθηκε το MLP Νευρωνικό Δίκτυο το οποίο μπορεί να
παραμετροποιηθεί και να εκτελεστεί με αρχείο `runMlp.java`.

+ Η παραμετροποίηση πραγματοποιείται δίνοντας τιμές στις μεταβλητές στο αρχείο
όπως φαίνεται παρακάτω:

```java
int numOfHiddenLayers = 3; // type "2" or "3"
int D = 2;
int H1 = 10;
int H2 = 8;
int H3 = 8; // Ignored if numOfHiddenLayers == 2
int K = 4;
String hiddenLayerActivationFunction = "tanh"; //type "relu" or "tanh"
double LEARNING_RATE = 0.0009;
int BATCH_SIZE = 1;
int MINIMUM_EPOCHS = 700;
double TERMINATION_THRESHOLD = 0.1;
```

+ Για την εκτέλεση:
  
```bash
$javac Neuron.java Mlp.java runMlp.java
$java runMlp 
```

Κατά την εκτέλεση του προγράμματος καλούνται τρεις βασικές συναρτήσεις:

```java
mlp.initWeights();
mlp.gradientDescent("training_set.txt");
mlp.testNetwork("test_set.txt");
```

+ Με την `initWeights()` αρχικοποιούνται όλα τα βάρη και οι πολώσεις του δικτύου
σε τυχαίους αριθμούς μεταξύ -1 και 1.
+ Με την `gradientDescent()` φορτώνεται το αρχείο που περιέχει τα δεδομένα εκπαίδευσης
του δικτύου και τρέχει ο αλγόριθμος για τις παραμέτρους που έχουν οριστεί. Για την
λειτουργία του αλγορίθμου έχουν υλοποιηθεί οι συναρτήσεις `forwardPass(double[] networkInput)`,
που δέχεται ως όρισμα μια είσοδο για το δίκτυο και επιστρέφει την έξοδο του και η
`backprop(double[] networkInput, double[] data_label)` η οποία δέχεται ως όρισμα μια είσοδο
και την επιθυμητή κατηγορία που θα πρέπει να επιστρέψει το δίκτυο, υπολογίζει το σφάλμα και
την μερική παράγωγο σε κάθε νευρώνα. Ανάλογα με τον αριθμό τον mini-batches που έχουμε δώσει
καλείται η συνάρτηση `updateWeights()` η οποία ενημερώνει τα βάρη και τις πολώσεις στο δίκτυο
χρησιμοποιώντας τον ρυθμό μάθησης που έχει οριστεί. Η εκπαίδευση τρέχει για το ελάχιστο των 700
εποχών (`MINIMUM_EPOCHS`) όπως ορίζεται και συνεχίζει να τρέχει μέχρι η διαφορά δύο διαδοχικών
σφαλμάτων να είναι μικρότερη απο το κατώφλι που ορίζεται στην αρχή του προγράμματος
(`TERMINATION_THRESHOLD`). Καθ' όλη την διάρκεια της εκτέλεσης τυπώνεται το συνολικό σφάλμα
εκπαίδευσης σε κάθε εποχή και κατά τον τερματισμό γράφονται όλα τα αποτελέσματα στο αρχείo
`mlp_output.txt`. Τέλος τυπώνεται ο χρόνος που χρειάστηκε για την ολοκλήρωση της εκπαίδευσης.
+ Με την `testNetwork()` φορτώνονται τα δεδομένα ελέγχου του δικτύου και ύστερα δίνονται ως είσοδο
συγκρίνοντας την έξοδο του με την επιθυμητή. Στο τέλος της εκτέλεσης της τυπώνεται το ποσοστό
των σωστών αποφάσεων στο σύνολο ελέγχου και γράφονται στο αρχείο `mlp_error.txt` τα παραδείγματα
ελέγχου με το σύμβολο "+" εάν το δίκτυο επέλεξε την σωστή κατηγορία για το συγκεκριμένο παράδειγμα
και "-" σε αντίθετη περίπτωση.

## **Συνάρτηση Εξόδου**

Για την συνάρτηση εξόδου του δικτύου χρησιμοποιείται η λογιστική συνάρτηση (sigmoid)
καθώς αυτή χρησιμοποιείται για την ταξινόμηση δεδομένων σε κατηγορίες.  

## **Βέλτιστο Δίκτυο που Παρατηρήθηκε**

Παρακάτω δίνονται οι παράμετροι που χρησιμοποιήθηκαν για το δίκτυο με το
μικρότερο σφάλμα γενίκευσης:

```java
int numOfHiddenLayers = 3; // type "2" or "3"
int D = 2;
int H1 = 10;
int H2 = 10;
int H3 = 8; // Ignored if numOfHiddenLayers == 2
int K = 4;
String hiddenLayerActivationFunction = "tanh"; //type "relu" or "tanh"
double LEARNING_RATE = 0.003;
int BATCH_SIZE = 1;
int MINIMUM_EPOCHS = 700;
double TERMINATION_THRESHOLD = 0.01;
```

</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

+ Η έξοδος:

![best_run](images/best_performance_output.png)

+ Η καμπύλη του σφάλματος προς κάθε εποχή:
  
![error-epoch-plot](images/error-epoch-plot.png)

Τα αρχεία εξόδου της παραπάνω εκτέλεσης είναι αποθηκευμένα με
ονόματα `mlp_error_final.txt` και `mlp_output_final.txt`.

</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

## **Παρατηρήσεις**

<ins>Μεταβολή αριθμού νευρώνων στα κρυμμένα επίπεδα</ins>

Μεταβάλλοντας τον αριθμό των νευρώνων σε δίκτυο με δύο κρυμμένα επίπεδα
παρατηρήθηκε πως η αύξηση στο πρώτο επίπεδο έχει μεγαλύτερη επίδραση στην
γενικευτική ικανότητα σε σύγκριση με την αύξηση στο δεύτερο κρυμμένο επίπεδο.
Ομοίως, σε δίκτυο με τρία κρυμμένα επίπεδα, παρατηρούνται όλο και μικρότερες
μεταβολές κατα την μεταβολή του πλήθος των νευρώνων από το πρώτο στο τρίτο
κρυμμένο επίπεδο.
Ακόμα να σημειωθεί πως με την προσθήκη του τρίτου κρυμμένου επιπέδου δεν
παρατηρήθηκε σημαντικό όφελος καθώς αυξανόταν ο χρόνος εκπαίδευσης και
χωρίς να υπάρχει σημαντική αύξηση στην γενικευτική ικανότητα.

<ins>Συνάρτηση ενεργοποίησης</ins>

Χρησιμοποιώντας τις δύο συναρτήσεις ενεργοποίησης παρατηρήθηκε πως με την χρήση της relu
μειώνεται ο χρόνος εκπαίδευσης του δικτύου μειώνοντας όμως την ικανότητα γενίκευσης.
Ακόμα παρατηρήθηκαν περιπτώσεις κατα τις οποίες πολλοί νευρώνες "νεκρώνονται",
δηλαδή έχουν συνεχόμενα έξοδο μηδέν με αποτέλεσμα να εμποδίζουν την εκπαίδευση
του δικτύου και μειώνουν την γενικευτική ικανότητα.
Χρησιμοποιώντας την υπερβολική εφαπτομένη ο χρόνος εκπαίδευσης αυξάνεται αρκετά
επιτυγχάνοντας όμως υψηλότερη ικανότητα γενίκευσης και αποφεύγοντας το πρόβλημα
που αναφέρθηκε παραπάνω.

<ins>Μέγεθος mini-Batch</ins>

Η αύξηση του μεγέθους mini-batch συμβάλλει στην μείωση του χρόνου εκπαίδευσης
μειώνοντας ωστόσο την γενικευτική ικανότητα του δικτύου.